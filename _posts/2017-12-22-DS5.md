---
layout: post
title: Distributed System V
subtitle: 15640 DS5 - Fault Tolerance
date: 2017-12-22
categories: Note
tags: [DS, 15640]
catalog: true
---

## Distributed System V

"Hi everyone, welcome back to Distributed System \\(//âˆ‡//)\ Today we will talk about Fault Tolerance in Distributed Systemâ€¦" Danielå¸¸ç”¨å¼€åœºç™½ã€‚

è¿™ä¸€ç¯‡ä¸»è¦å…³äºåˆ†å¸ƒå¼ç³»ç»Ÿä¸‹çš„å®¹é”™ï¼Œä»‹ç»äº†æ¯”å¦‚Checkpointï¼ŒLogging & Recoveryï¼ŒReplicationç­‰å’ŒRAIDã€‚è¿™ä¸€éƒ¨åˆ†åœ¨DSä¸­è¦æ±‚é‡ç‚¹æŒæ¡PAXOSã€‚å¯æƒœæ²¡æœ‰PAXOSè¿™ä¸ªprj, RAFTä¹Ÿè¿˜è¡Œå§å””ã€‚

### Logging and Recovery

è¿™ä¸€éƒ¨åˆ†Danielè®²äº†, åˆ†å¸ƒå¼ç³»ç»Ÿä¸‹çš„å®¹é”™æœºåˆ¶ã€‚åœ¨å‡ºç°éƒ¨åˆ†é”™è¯¯çš„æƒ…å†µä¸‹ï¼Œç³»ç»Ÿåº”è¯¥æ˜¯å¯é çš„ã€‚å¦‚ä½•å®šä¹‰å¯é å‘¢ï¼Ÿ~~emmm, é¢œå³æ­£ä¹‰ï¼Œå‘¸~~ (Availability, Reliability, Safely, Maintainability)

#### Dependability Concepts

Availability å¯ç”¨æ€§: the system is ready to be used immediately

Reliability å¯é æ€§: the system runs continuously without failure

Availiability å’Œ Reliability ä¹Ÿæ˜¯tradeoff. æ¯”å¦‚ä¸€ä¸ªä½MTTFçš„ç³»ç»Ÿï¼Œå°±ä¼šæœ‰ä½å¯ç”¨æ€§ï¼Œå› ä¸ºå®ƒå…·æœ‰é«˜å¯é æ€§ã€‚

Safety å®‰å…¨æ€§: if a system fails, nothing catastrophic will happen

Maintainability (Recovery) å¯ç»´æŠ¤æ€§: when a system fails, it can be repaired easily and quickly

**Failure Models**

Crash/Omission/Timing/Response/Arbitrary(Byzantine)

#### Redundancy

Information Redundancy: add extra bits to allow for error detection/recovery

Time Redundancy: perform operation and, if needs be, perform it again.

Physical Redundancy: add extra (duplicate) hardware and/or software to the system

#### Checkpoint

**Backward recovery**: return the system to some previous correct state (using checkpoints), then continue executing (Common one)

* Checkpointing can be very expensive (especially when errors are very rare)

**Forward recovery**: bring the system into a correct new state, from which it can then continue to execute

* Harder to know howdo to bring the system forward to a correct state.

  å¦‚æœfailureå‘ç”Ÿï¼Œç³»ç»Ÿå¯ä»¥é€šè¿‡Checkpointæ¢å¤åˆ°ä¸€ä¸ªè¾ƒæ—©çš„çŠ¶æ€ã€‚åœ¨å¤šçº¿ç¨‹èƒŒæ™¯ä¸‹Checkpointå‡ºç°å¤šçº¿ç¨‹Transactionï¼Œè¿™ä¸ªCheckpointå°±æ˜¯inconsistentçš„ã€‚æ‰€ä»¥å‡ºç°äº†Coordinated Checkpointingè¿™ä¸ªæ¦‚å¿µã€‚

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/cp.png" style="zoom:45%">

**Coordinated Checkpointing**

Key idea: each process takes a checkpoint after a globallycoordinated action

Simple Solution: 2-phase blocking protocol

Optimization: consider only processes that depend on therecovery of the coordinator

| Successful Coord                         | Unsuccessful Coord                       |
| ---------------------------------------- | ---------------------------------------- |
| <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/cp1.png" style="zoom:45%"> | <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/cp2.png" style="zoom:45%"> |

#### Logging and Recovery				

Goal: make transaction reliable.

General idea: store enough information to disk to determine global state

Challenges: disk performance is poor,  writing to disk to handle arbitrary crash is hard -> Shadow pages and Write-ahead Logging (WAL), providing Atomicity and Durability

##### Shadow Paging Vs WAL

|        | WAL                                     | Shadow Page                              |
| ------ | --------------------------------------- | ---------------------------------------- |
| ACID   | A, D                                    | A, D. page = unit of storage             |
| Idea   | create log recording every update to db | When write a page, make a shadow copy    |
| ABORT  | recover by replaying log                | discard shadow page                      |
| COMMIT | LOG typically store both REDO and UNDO  | make shadow page real                    |
| Other  | Update versions kept in memory          | "copy-on-write" to avoid in-place page update |

* WAL is more common, fewer disk operations, transactions considered committed once log written.

#### ARIES

ARIES: Algorithms for Recovery and Isolation Exploiting Semantics 

Principles 

* Write-ahead logging
* Repeating history during Redo
* Logging changes during Undo

Write-Ahead Logging

* Pages on disk, some also in memory (page cache)
  *  â€œDirty pagesâ€: page in memory differs from one on disk

- Reconstruct global consistent state using 
  - Log files + disk contents + (page cache)

ä¸€æ¡LOGåŒ…å«å¾ˆå¤šä¿¡æ¯: LSN: [prevLSN, TID, â€œupdateâ€, pageID, new value, ol
value]

LSN: Log-Sequence Number, in order

TID, prevLSN: PrevLSN forms a backward chain of operations for each TID 

TT: Transaction Table,  all TXNS not written to disk

DPT: Dirty Page Table,  all dirty pages in memory									

**Recovery using WAL**

Analysis Pass

* Reconstruct TT and DPT (from start or last checkpoint)
* Get copies of all pages at the start

Recovery Pass (redo pass)

* Replay log forward, make updates to all dirty pages
* Bring everything to a state at the time of the crash

Undo Pass

- Replay log file backward, revert any changes made by transactions that had not committed (use PrevLSN)
- For each write Compensation Log Record (CLR)
- Once reach entry without PrevLSNïƒ done

åˆ©ç”¨WALæ¥è¿›è¡Œä¿®å¤ï¼Œé¦–å…ˆæ˜¯åˆ†æï¼Œåœ¨è¿™ä¸€æ­¥ä¸­è¿˜åŸDPTå’ŒTTåˆ°crashçš„çŠ¶æ€ã€‚é€šè¿‡æµè§ˆlogæ–‡ä»¶ä»å¼€å§‹åˆ°Cheakpointçš„éƒ¨åˆ†ï¼Œå°†æ‰€æœ‰çš„äº‹åŠ¡åŠ åˆ°TTä¸­ï¼Œåœ¨TTä¸­ä¿æŒTIDå’ŒLastLSNï¼Œæ‰¾åˆ°REDOçš„å¼€å§‹ã€‚åŒæ—¶å°†è¿›è¡Œäº†æ”¹åŠ¨çš„pageåŠ å…¥åˆ°DPTä¸­ï¼Œåœ¨DPTä¸­ä¿æŒpageIDå’ŒrecoveryLSNã€‚åœ¨TTä¸­LastLSNè®°å½•æœ€æ–°çš„æ”¹åŠ¨ï¼ŒDPTä¸­recoveryLSNè®°å½•çš„æ˜¯æœ€åˆçš„æ”¹åŠ¨ã€‚

Redoè¿‡ç¨‹ä¸­ï¼Œé€šè¿‡æ£€æŸ¥DPTå’Œlogæ–‡ä»¶ï¼Œå¯ä»¥æ‰¾åˆ°æ‰€æœ‰æ²¡æœ‰commitåˆ°diskçš„äº‹åŠ¡ã€‚å·²ç»æˆåŠŸæ·»åŠ åˆ°ç¡¬ç›˜çš„æ”¹åŠ¨æ˜¯ä¸éœ€è¦Redoçš„ã€‚Redoçš„è¿‡ç¨‹åŒæ ·ä¿®æ”¹TTï¼Œå¹¶ä¸”ä¿è¯æ¯ä¸ªTransactionä¸ä¼šè¢«å¤„ç†ä¸¤æ¬¡ã€‚

åœ¨Redoè¿‡ç¨‹ä¸­ï¼Œå½“å‰çš„çŠ¶æ€å·²ç»è¢«æ›´æ”¹ä¸ºå‡ºç°é”™è¯¯æ—¶çš„çŠ¶æ€ã€‚Undoè¿‡ç¨‹å°†å½“å‰æ²¡æœ‰Commitçš„Transactionæ¢å¤ã€‚å¯ä»¥çœ‹ä½œæ˜¯Backward recoveryçš„è¿‡ç¨‹ã€‚åœ¨TTä¸­åˆ©ç”¨LastLSNï¼Œå¯¹äºæ¯ä¸ªrecordæ¢å¤æ”¹åŠ¨å¹¶ä¸”åŒæ—¶å¢åŠ æ–°çš„logæ–‡ä»¶ã€‚

é¡ºä¾¿è¯´WALæ˜¯å¯ä»¥å’Œ2PCæ­é…ä½¿ç”¨çš„ã€‚



### Distributed Replication

è¿™èŠ‚è®²å®¹é”™æŠ€æœ¯ä¸­çš„æ‹·è´ã€‚ä¹‹å‰è®²çš„RecoveryæŠ€æœ¯ç”±äºç­‰å¾…æ—¶é—´é•¿ï¼Œæ‰€ä»¥åœ¨å‡ºç°é”™è¯¯çš„æ—¶å€™ä¸ºäº†ä¿æŒç³»ç»Ÿçš„è¿è¡Œï¼Œæ‹·è´æ˜¯å¿…è¦çš„ã€‚

Goal: Stay up during failures

å¯¹äºæ‹·è´ï¼Œread-onlyçš„æ•°æ®æ˜¯å¾ˆå®¹æ˜“çš„ï¼Œåªéœ€è¦æ‹·è´å¾ˆå¤šå°±å¯ä»¥äº†ã€‚ä½†æ˜¯read-writeçš„æ•°æ®è¿˜æ¶‰åŠåˆ°ä¸€è‡´æ€§çš„é—®é¢˜ã€‚è¯´åˆ°ä¸€è‡´æ€§...

#### Consistency

**Strict Consistency**

* Read always returns value from latest write

**Sequential Consistency**

* All nodes see operations in some sequential order
* Operations of each process appear in-order in this sequence

| Sequential Consistency                   | Violate Sequential Consistency           |
| ---------------------------------------- | ---------------------------------------- |
| <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/seq1.png" style="zoom:45%"> | <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/seq2.png" style="zoom:45%"> |

**Causal Consistency**

* All nodes see causally related writes in same order
* But concurrent writes may be seen in different order on different machines

| Causal Consistency                       | Violate Causal Consistency               |
| ---------------------------------------- | ---------------------------------------- |
| <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/caul1.png" style="zoom:45%"> | <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/caul2.png" style="zoom:45%"> |

**Eventual Consistency**

* All nodes will learn eventually about all writes, in the absence of update

#### Replication Strategies

* Propagate only a notification of an update
* Transfer data from one copy to another
* Propagate the update operation to other copies

**When to replicate**

Pull based: Replicas/Clients poll for updates (caches)

Push based: Server pushes updates (stateful)

#### Primary-backup replication model

**Assumptions**

* Group membership manager
* Fail-stop(not Byzantine) failure model
* Failure detector

**Primary-Backup Write Protocol**

| Remote                                   | Local                                    |
| ---------------------------------------- | ---------------------------------------- |
| <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/pb1.png" style="zoom:45%"> | <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/pb2.png" style="zoom:45%"> |

* Advantages: With N servers, can tolerate loss of N-1 copies
* Must wait for failure detector

#### Quorum Based Consensus

é‡ç‚¹PAXOS

* Designed to have fast response time even under failures
* Operate as long as majority of machines is still alive
* To handle f failures, must have 2f + 1 replicas
* For replicated-write => write to all replicaâ€™s not just one

##### PAXOS

**Components**

* Proposers:
  * Active: put forth particular values to be chosen
  * Handle client requests
* Acceptors:
  * Passive: respond to messages from proposers
  * Responses represent votes that form consensus
  * Store chosen value, state of the decision process

**Single Decree Paxos**

* Phase 1: Prepare message
  * Find out about any chosen values
  * Block older proposals that have not yet completed
* Phase 2: Accept message
  * Ask acceptors to accept a specific value
* (Phase 3): Proposer decides
  * If majority again: chosen value, commit.
  * If no majority: delay and restart Paxos

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/paxos.png" style="zoom:50%">

Paxosæ˜¯éå¸¸é‡è¦çš„ä¸€ä¸ªProtocal, åœ¨å·¥ä¸šç•Œä¹Ÿæœ‰å¾ˆå¹¿æ³›çš„åº”ç”¨ã€‚é™¤äº†Single-Paxos, è¿˜æœ‰Multi-Paxosä½†æ˜¯æ²¡æœ‰é‡ç‚¹ä»‹ç»ã€‚

æ€»ä¹‹è¿™ä¸€èŠ‚ä¸»è¦ä»‹ç»äº†Primary-backupå’ŒQuorum consensusä¸¤ç§æ‹·è´çš„æ–¹å¼ã€‚å…¶ä¸­Paxosæ˜¯é‡ç‚¹ï¼Œåœ¨ååŠæœŸDSçš„å­¦ä¹ ä¸­ä¹Ÿä¼šæœ‰æ‰€åº”ç”¨ã€‚

| Primary-backup                           | Quorum consensus                         |
| ---------------------------------------- | ---------------------------------------- |
| Replicas are "passive", follow primary   | Replicas are "active", participate in protocol, no master |
| Simple. N machines, can handle N-1 failures | Complex. To handle f failures, need 2f+1 replicas |
| Slow responses times in case of failures | Clients don't see the failures           |



### Errors and RAID

è¿™ä¸€èŠ‚ä¸»è¦ä»‹ç»äº†ç£ç›˜å†—ä½™é˜µåˆ—RAIDï¼Œä»¥åŠè¯„ä¼°ç³»ç»Ÿå¯ç”¨æ€§çš„MTTFè®¡ç®—ã€‚

**Type of Errors**

Hard errors: Component is dead

Soft errors: A signal or bit is wrong, but doesn't mean component is faulty

#### Meausuring Availability

**Availability = MTBF/(MTBF + MTTR) **

MTBF: Mean Time Between Failure

MTTR: Mean Time To Repair

MTTF: Mean Time To Failure

MTBF = MTTF + MTTR

#### Error Detection

ä»‹ç»å‘ç°é”™è¯¯çš„ä¸€äº›æ–¹æ³•ã€‚

**Parity Checking**

å¥‡å¶æ ¡éªŒï¼Œä¹‹å‰å­¦é€šä¿¡ä¹Ÿæ¥è§¦è¿‡å¥½å‡ æ¬¡ã€‚èƒ½å¤Ÿæ£€æµ‹å¥‡æ•°æ¬¡bité”™è¯¯çš„å‘ç”Ÿï¼Œæ— æ³•å®šä½é”™è¯¯ä½ç½®ã€‚

**Block Error Detection**

å¹¶ä¸æ˜¯æ‰€æœ‰çš„é”™è¯¯éƒ½èƒ½è¢«æ¢æµ‹ã€‚	

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/bed.png" style="zoom:50%">

EDC: Error Detection Code	

**CheckSum**

æ£€æŸ¥packetå†…æ‰€æœ‰çš„words/shorts/bytesçš„å’Œã€‚åº”ç”¨å¹¿æ³›ï¼Œå®ç°ç®€å•ï¼Œå¾ˆå¼±ï¼Œè®¸å¤šè¯¸å¦‚bitäº¤æ¢è¿™ç§é”™è¯¯å°±æ— æ³•æ£€æµ‹...

**Cyclic Redundancy Check**

å¾ªç¯å†—ä½™æ£€æŸ¥ï¼Œå¯¹äºdæ¯”ç‰¹çš„æ•°æ®Dï¼Œé€‰æ‹©r+1æ¯”ç‰¹çš„Gï¼Œè®¡ç®—ræ¯”ç‰¹çš„CRCæ·»åŠ åˆ°Dåï¼Œå¾—åˆ°æ•°æ®\<D, R\>ã€‚\<D, R\>å¯ä»¥è¢«Gæ•´é™¤ã€‚å‘é€\<D, R\>, å¦‚æœæ¥æ”¶ç«¯æ— æ³•æ•´é™¤G, è¯æ˜å‡ºç°äº†é”™è¯¯ã€‚å…¶ä¸­Gæ˜¯äº‹å…ˆç”±å‘é€å’Œæ¥æ”¶æ–¹è¾¾æˆä¸€è‡´çš„ã€‚å¯ä»¥æ£€æµ‹å‡ºå°äºr+1æ¯”ç‰¹çš„é”™è¯¯ã€‚

ä¸¾ä¸ªä¾‹å­, D = 101110, G = 1001, r = 3. R = reminder[(D<<\<3)/G] = 011. å‘é€\<D, R\> = 101110011.

#### Error Recovery

æ£€æµ‹å‡ºé”™è¯¯ä¹‹åæ˜¯é”™è¯¯çš„æ¢å¤ã€‚ä¸»è¦åˆ†ä¸¤ç§æƒ…å†µï¼Œä¸€ç§æ˜¯åˆ©ç”¨å†—ä½™æ¢å¤ï¼Œå¦ä¸€ç§æ˜¯é‡æ–°å°è¯•ç›´åˆ°æˆåŠŸã€‚

**Error Correcting Codes**

ä¸¾ä¸ªç”¨å¥‡å¶æ ¡éªŒç æ¢å¤é”™è¯¯çš„ğŸŒ°ï¼Œåªèƒ½æ­£ç¡®æ¢å¤1bité”™è¯¯ã€‚

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/ecc.png" style="zoom:50%">

**Replication & Voting**

æ‰€æœ‰æ‹·è´æŠ•ç¥¨ï¼Œé€‰æ‹©majorityã€‚å¥½åƒå¾ˆä¸å¯é çš„æ ·å­ï¼Œå› æ­¤ä¸€èˆ¬åœ¨éœ€è¦é«˜å¯ç”¨æ€§çš„ç³»ç»Ÿä¸­ä½¿ç”¨ã€‚

**Retry**

é‡æ–°å‘ä¸€éå¾—åˆ°ACKï¼Œéœ€è¦é”™è¯¯æ£€æµ‹æœºåˆ¶ã€‚

#### Fault Tolerance Design

å¦‚ä½•å°†é”™è¯¯æ‰¼æ€åœ¨æ‘‡ç¯®é‡Œã€‚

èƒŒæ™¯ï¼šUse multiple disks

* Capacity
  * More disks allows us to store more data
* Performance
  * Access multiple disks in parallel
  * Each disk can be working on independent read or write
  * Overlap seek and rotational positioning time for all
* Reliability
  * Recover from disk (or single sector) failures
  * Will need to store multiple copies of data to recover

**Disk Striping ç¡¬ç›˜åˆ†å‰²**: Interleave data across multiple disks

* Large file streaming can enjoy parallel transfers
* Small requests benefit from load balancing

  æŠŠæ•°æ®äº¤é”™çš„å­˜åœ¨ä¸åŒçš„ç¡¬ç›˜ä¸Šï¼Œå¯ä»¥å¹¶è¡Œçš„è®¿é—®æ•°æ®ï¼Œå¹¶ä¸”å¯ä»¥ä¿è¯æ•°æ®æµå¹³è¡¡ã€‚			

##### JBOD

Just a bunch of disks. å°±æ˜¯ä¸€å¤§å †ç¡¬ç›˜ï¼Ÿåå­—ä¸é”™...

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/jbod.png" style="zoom:50%">

ä¸€å¤§å †ç¡¬ç›˜ï¼Œæ¯ä¸ªç¡¬ç›˜ä¹‹é—´æ²¡æœ‰å…³ç³»ï¼Œå¦‚æœå‡ºç°äº†é”™è¯¯æ— æ³•æ¢å¤ã€‚

ä¸ºäº†æ•°æ®çš„å®‰å…¨ï¼Œéœ€è¦å¿…è¦çš„å†—ä½™æ¥è¿›è¡Œå¤‡ä»½å’Œæ£€æµ‹ï¼ä¿®å¤é”™è¯¯ã€‚æ¯”è¾ƒå¸¸è§çš„æ•°æ®å†—ä½™åŒ…æ‹¬ï¼šreplication æ‹·è´/ erasure-correcting codes é”™è¯¯ç§»é™¤ä»£ç / error-detecting codes é”™è¯¯æ£€æµ‹ä»£ç ã€‚å¯¹äºæ‹·è´ï¼Œå†™æ“ä½œå†™å…¥æ‰€æœ‰çš„æ‹·è´ï¼Œè¯»æ“ä½œä»ä»»æ„ä¸€ä¸ªè¯»å–ã€‚

##### Parity Disk

ä¸€ä¸ªç¡¬ç›˜ç”¨æ¥å­˜æ‰€æœ‰çš„å¥‡å¶æ ¡éªŒç ï¼Œè‡ªæ£€æµ‹ã€‚

Capacity: one extra disk save per stripe	
Erasures: disk failures aer self-identifying

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/pd.png" style="zoom:50%">

å¯¹äºå¥‡å¶æ ¡éªŒç å’Œæ‹·è´çš„å­˜åœ¨ï¼Œæ•°æ®çš„æ— é”™è¯¯è¯»æ“ä½œå¯ä»¥å¹¶è¡Œè¯»å–ï¼Œå†™æ“ä½œè¦æ›´æ–°å¥‡å¶æ ¡éªŒç ã€‚å¯ä»¥å¿å—ä¸€ä¸ªé”™è¯¯ã€‚ç”±äºéœ€è¦å¤šæ¬¡è¯»ï¼å†™å¥‡å¶æ ¡éªŒç çš„ç¡¬ç›˜ï¼Œload balancedä¸å­˜åœ¨çš„ï¼Œä¼šæˆä¸ºbottleneckã€‚è§£å†³æ–¹å¼å°±æ˜¯å°†å¥‡å¶æ ¡éªŒç å¹³å‡çš„åˆ†å¸ƒåˆ°æ¯ä¸€ä¸ªç¡¬ç›˜ä¸Šã€‚

**Striping Parity**

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/spd.png" style="zoom:50%">

##### RAID

ä¸»è¦ä»‹ç»RAID0, 1, 4, 5.

| RAID0: Striping                          | RAID1: Mirroring                         | RAID4: Parity                            | RAID5: Rotated Prity                     |
| ---------------------------------------- | ---------------------------------------- | ---------------------------------------- | ---------------------------------------- |
| Performance\*4, Capacity\*4, Reliability*1/4, Survive\*0 | Performance:R\*2 W<1, Capacity\*2, Survive*1 | 4 I/Os per Write, Performance: R*3 W<1, Capacity\*3, Survive\*1 | 4 I/Os per Write, Performance: R*3 W<1, Capacity\*3, Survive\*1 |
| <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/raid0.png" style="zoom:50%"> | <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/raid1.png" style="zoom:50%"> | <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/raid4.png" style="zoom:50%"> | <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/raid5.png" style="zoom:50%"> |
| Performance and capacity really matter but reliability doesn't | Reliability and write performance  matter, but capacity doesn't | ~~The same as RAID5, why not   using RAID5~~ | When capacity and cost matter or workload is read-mostly |

**Compare**<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/raidCompare.png" style="zoom:50%">



#### Availability/Reliability Metrics

å¯é æ€§/å¯ç”¨æ€§ç›¸å…³è®¡ç®—ï¼Œè®¡ç®—MTTDL(Mean Time To First Data Loss)ï¼Œæœ¬èŠ‚çš„è®¡ç®—éƒ½æ˜¯ä¼°ç®—ã€‚

MTTF = mean time between failures (for each disk)

MTTDL = mean time to first disk failure (for each system)

MTTDLè¶Šå¤§ï¼Œå¯é æ€§è¶Šé«˜ï¼Œå¯ç”¨æ€§è¶Šä½ã€‚

**Reliability without rebuild**

Stripæƒ…å†µï¼Œç³»ç»Ÿçš„MTTDL = ç¡¬ç›˜MTTFï¼ ç¡¬ç›˜æ€»æ•°ã€‚

Mirroræƒ…å†µï¼Œç³»ç»ŸMTTDL = 1.5\*ç¡¬ç›˜MTTFã€‚

ä¸¾å‡ ä¸ªğŸŒ°å¦‚ä¸‹ã€‚

* 200 data drives with MTTF driveï¼š MTTDLarray = MTTFdrive / 200
* Add 200 drives and do mirroring
  * MTTFpair = (MTTFdrive / 2) + MTTFdrive = 1.5 * MTTFdrive
  * MTTDLarray = MTTFpair / 200 = MTTFdrive / 133
* Add 50 drives, each with parity across 4 data disks
  * MTTFset = (MTTFdrive / 5) + (MTTFdrive / 4) = 0.45 * MTTFdrive
  * MTTDLarray = MTTFset / 50 = MTTFdrive / 111

**Reliability with rebuild**

å¥‡å¶æ ¡éªŒç å’Œé•œåƒåˆ†åˆ«å¯ä»¥å®¹å¿ä¸€ä¸ªé”™è¯¯ã€‚å¯¹äºé•œåƒï¼Œä»æ­£ç¡®çš„æ‹·è´è¯»å–æ•°æ®ã€‚å¯¹äºå¥‡å¶æ ¡éªŒç ï¼Œå·²çŸ¥é”™è¯¯ç¡¬ç›˜ï¼Œå¯ä»¥åˆ©ç”¨æ ¡éªŒç XORå…¶ä»–ç¡¬ç›˜æ•°æ®æ¨æ–­å‡ºæ­£ç¡®æ¯”ç‰¹ã€‚éœ€è¦æ»¡è¶³åœ¨æ²¡æœ‰æ•°æ®ä¸¢å¤±å’Œåœ¨ç¬¬äºŒä¸ªé”™è¯¯å‘ç”Ÿä¹‹å‰ä¿®å¤ç¬¬ä¸€ä¸ªé”™è¯¯ã€‚

* MTTDLarray = MTTFfirstdrive * (1 / prob of 2nd failure before repair) 
  * prob is MTTRdrive / MTTFseconddrive
* Mirroring: MTTDLpair = (MTTFdrive / 2) * (MTTFdrive / MTTRdrive)
* 5-disk parity-protected arrays: MTTDLset = (MTTFdrive / 5) * ((MTTFdrive / 4 )/ MTTRdrive)







å‚è€ƒèµ„æ–™ï¼š~~Markdownçš„æ ¼å¼å¤ªä¸‘äº†èƒ½æ”¹è¡Œé—´è·å—å¥½åƒä¸èƒ½ç”¨html+cssä¹Ÿå¤ªéº»çƒ¦äº†æˆ‘å°±æ˜¯æƒ³å†™ä¸ªç¬”è®°è€Œå·²å•Š~~

[ARIES](https://en.wikipedia.org/wiki/Algorithms_for_Recovery_and_Isolation_Exploiting_Semantics)

[Paxos](http://the-paper-trail.org/blog/consensus-protocols-paxos/)

[Raft](https://raft.github.io)

[Paxos Made Simple](https://people.cs.umass.edu/~arun/cs677/reading/paxos-simple.pdf)

[RAIDs](http://pages.cs.wisc.edu/~remzi/Classes/537/Spring2011/Book/file-raid.pdf)





~~å¿ä¸ä½æ¯ä¸€ç¯‡éƒ½è¡¨ç™½Danielä½†æ˜¯ä¸€æƒ³åˆ°åˆä¸èƒ½è·Ÿä»–å·¥ä½œç®€ç›´(;Â´à¼àº¶Ğ”à¼àº¶`) åˆ°è¿™é‡Œä¸ºæ­¢æœŸä¸­è€ƒè¯•ä¹‹å‰çš„éƒ¨åˆ†å°±ç»“æŸäº†ã€‚å¦‚æœè¶³å¤Ÿå‰å®³DSèƒ½Açš„è¯å°±å¯ä»¥ç”³è¯·å’ŒDanielå®éªŒå®¤æ‰“å·¥äº†å§ è¿™å±ŠDSä¸åˆ°92+ä¸å¯èƒ½A ä¸æ„§æ˜¯CMU æŠ€ä¸å¦‚äººç”˜æ‹œä¸‹é£å†è§Daniel~~
