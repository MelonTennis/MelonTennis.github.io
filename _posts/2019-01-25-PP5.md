---
dzzlayout: post
title: Parallel Architecture & Programing V
subtitle: 15618 PP5 Performance Evaluation
date: 2019-01-25
categories: Note
tags: [PP]
catalog: true
header-img: "img/ctc1.jpeg"
---

## Parallel Architecture & Programing ä¼

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

~~å·¥ä½œä½¿æˆ‘è‡ªé—­ï¼Œä¸§ä¸§ä¸§ã€‚ ç”šè‡³ä¸çŸ¥é“åˆ°åº•æ˜¯dataçƒ‚ï¼Œè¿˜æ˜¯æˆ‘çƒ‚ã€‚ç®—äº†ï¼Œå°±ç®—æˆ‘çš„å§ï¼Œéšä¾¿äº†ã€‚ä»Šå¤©å¬è¯´æˆ‘ç ”ç©¶ç”Ÿæ—¶æœŸé¼“åŠ±è¿‡æˆ‘çš„å­¦é•¿å›å›½äº†ï¼Œå¯æƒœäº†äº†è¿˜æ²¡å½“é¢æ„Ÿè°¢å‘¢ï½å¤§å®¶çº·çº·å›å›½è¿‡å¹´ï¼Œå¿«ä¹éƒ½æ˜¯åˆ«äººçš„æˆ‘å•¥éƒ½æ²¡æœ‰ğŸ¶~~

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/baddata.png" style="zoom:65%">

### Size for scaling

* There can be complex interactions between the size of the problem and the size of the parallel computer
  * Can impact load balance, overhead, arithmetic intensity, locality of data access
  * Effects can be dramatic and application dependent

* Can be desirable to scale problem size as machine sizes grow

æ¯”å¦‚è¿™ä¸¤ä¸ªå›¾ï¼š

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/sizeproblem.png" style="zoom:70%">

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/sizeproblem1.png" style="zoom:65%">

* Too small a problem
  * Parallelism overheads dominate parallelism benefits
  * Problem size may be appropriate for small machines, but inappropriate for large ones

* Too large a problem
  * Key working set may not â€œfitâ€ in small machine
  * When problem working set â€œfitsâ€ in a large machine but not small one, super-linear speedups can occur

**Common Scaling Terminology**

* Strong scaling
  * Runtime of X on P processors, vs. of X on 1 processor
  * Goal = P
* Weak scaling
  * Runtime of (P*X) on P processors, vs. of X on 1 processor
  * Goal = 1.0

#### Scaling constraints

**Application-oriented scaling properties**

* Particles per processor
* Transactions per processor

**Resource-oriented scaling properties**

* Problem constrained scaling
  * Use a parallel computer to solve the same problem faster
  * Speedup =  $$\frac{time\ 1\  processor}{time\ P\ processors}â€‹$$
* Time constrained scaling
  * Completing more work in a fixed amount of time
  * Execution time kept fixed as the machine (and problem) scales
  * Speedup =  $$\frac{work\ done\ by\ P\  processors}{work\ done\ by\ 1\ processor}â€‹$$
* Memory constrained scaling
  * Run the largest problem possible without overflowing main memory
  * Memory per processor is held fixed
  * Neither work nor execution time are held constant
  * Speedup =  $$\frac{work(P\  processors)\ *\ time(1\ processor)}{time(P\  processors)\ *\ work(1\ processor)}$$ = $$\frac{work\ per\ unit\ time\ on\ P\ processors}{work\ per\ unit\ time\ on\ 1\ processor}â€‹$$

 è¿˜æ˜¯é‚£ä¸ª2D grid solverçš„ä¾‹å­ï¼š

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/grid2dscale.png" style="zoom:70%">

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/grid2dscale1.png" style="zoom:70%">

~~æƒ³èµ·æ¥è€ƒå‰å¤ä¹ ç®—äº†å¥½ä¹…è¿™äº›ï¼Œæ¯ä¸ªéƒ½ç®—äº†orz æ‰€ä»¥è¿™ä¸ªæ•…äº‹å‘Šè¯‰æˆ‘ä¸è®ºå­¦çš„å¤šå·®åªè¦ä¸æ”¾å¼ƒè‡ªå·±è¿˜æ˜¯èƒ½è¿‡çš„ï¼ˆæ‰“Aæ˜¯ä¸èƒ½æ‰“Aäº†ï¼ŒPPæˆ‘èƒ½æ‰“Aï¼ŒğŸ·éƒ½èƒ½ä¸ŠğŸŒ²~~

**Challenges of scaling**

* Preserve ratio of time spent in different program phases
* Preserve important behavioral characteristics
* Preserve contention and communication patterns
* Preserve scaling relationships between problem parameters

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/scaleadvice.png" style="zoom:65%">

~~æƒ³å›å®¶è¿‡å¹´ã€‚~~