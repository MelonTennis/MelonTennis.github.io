---
dzzlayout: post
title: Parallel Architecture & Programing III
subtitle: 15618 PP3 - Parallel Performance and Optimization
date: 2018-10-10
categories: Note
tags: [PP]
catalog: true
header-img: "img/ctc1.jpeg"
---

## Parallel Architecture & Programing å

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

ä¸çŸ¥é“è‡ªå·±åœ¨å¹²å˜›ï¼Œå¤§æ¦‚æ˜¯å‡ºç°äº†é”™è§‰ã€‚ \_(:Ğ·ã€âˆ )\_   æ—¥å¸¸é”™è§‰

ä¸ç®¡é”™è§‰çš„äº‹ï¼Œè¿™ç¯‡å¤ä¹ ä¸€ä¸‹performance optimization.

### Balancing workload

Ideally: all processors are computing all the time during program execution

**Key goals**

* balance workload onto available execution resources
* reduce communication (avoid stalls)
* reduce extra work (overhead)

**Static assignment**

* assignment of work to threads is pre-determined
* two approaches: blocked and interleaved
* simple and zero runtime overhead
* when cost/amount of work is predictable
* "semi-static" assignment - periodically profiles and re-adjusts

**Dynamic assignment**

* determines assignment dynamically at runtime - not pre-determined
* workload balance and overhead (sync cost) trade-off
* smarter scheduling - long tasks first
* stealing - increase locality
  * child stealing, continuation stealing
  * gready join scheduling policy

#### Fork-join parallelism

Natural way to express independent work in [divide-and-conquer](https://en.wikipedia.org/wiki/Divide_and_conquer_algorithm) algorithmsï¼ˆä¸è®¤è¯†åˆ†æ²»æ³•çš„æˆ‘orz

~~æ—¥å¸¸å¤±å¿†çš„æˆ‘å¹¶ä¸è®°å¾—è®²äº†è¿™ä¸ªfork-joinçš„å¹¶è¡Œæ¨¡å¼å¤§æ¦‚çœ‹èµ·æ¥æ˜¯è¿™æ ·çš„æˆ‘çœŸçš„ä¼šè¿‡å—~~

Fork: create new logical thread of control

* çˆ¶çº¿ç¨‹è°ƒç”¨ï¼Œåˆ›é€ æ–°çš„å¹¶å‘å­çº¿ç¨‹
* å­çº¿ç¨‹å’Œçˆ¶çº¿ç¨‹åˆ†åˆ«å¼‚æ­¥æ‰§è¡Œ

Join: returns when current function calls have completed 

* å­çº¿ç¨‹ç»“æŸæ—¶è°ƒç”¨
* çˆ¶çº¿ç¨‹ç­‰å¾…å­çº¿ç¨‹ç»“æŸ

å‘ç°[è¿™ä¸ªåˆ«å®¶çš„è¯¾ä»¶](http://ipcc.cs.uoregon.edu/lectures/lecture-9-fork-join.pdf)è¯¦ç»†ä»‹ç»äº†[fork-join model](https://en.wikipedia.org/wiki/Forkâ€“join_model) 

### Minimizing communication costs

~~ï¼ˆä¸‰ä¸ªæœˆå~~

~~æˆ‘ä¹Ÿæƒ³å›å®¶è¿‡å¹´å‘€ï½ ä»Šå¤©æŠŠä¹°äº†2ä¸ªæœˆçš„æ‰‹åŠæ‹†å¼€æ‘†ä¸Šäº†çœŸçš„å¥½å¥½çœ‹å•Šã€‚ä¸è¿‡æ‰‹åŠä¹°å¤šäº†ä¹Ÿæ²¡ç”¨ï¼Œç­‰é¢„è´­çš„è¿¹éƒ¨æ™¯å¾åˆ°äº†å¯èƒ½ä¹Ÿä¸ä¼šå†ä¹°äº†ã€‚ï¼ˆçœŸé¦™é¢„è­¦ï¼Ÿ~~

#### Message passing solver

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/ghostcell.png" style="zoom:65%">

 Message passing modelçš„ä¸€ä¸ªä¾‹å­ï¼Œdataè¢«åˆ†æˆå‡ ä¸ªéƒ¨åˆ†ï¼Œæ¯ä¸€ä¸ªéƒ¨åˆ†åœ¨ä¸€ä¸ªçº¿ç¨‹çš„åœ°å€ç©ºé—´ä¸­ï¼Œæ¯ä¸ªthreadå®Œæˆæœ¬åœ°è®¡ç®—åè¦å’Œç›¸é‚»threadè¿›è¡Œæ•°æ®äº¤æ¢ã€‚

**Synchronous (blocking) send & receive**

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/syncblock.png" style="zoom:65%">

é˜»å¡å‹é€šä¿¡å‡½æ•°ï¼Œsend()åœ¨senderæ”¶åˆ°receiverå‘é€çš„ackæ—¶è¿”å›ã€‚recv()åœ¨dataè¢«å¤åˆ¶åˆ°receiverçš„åœ°å€ç©ºé—´æ—¶è¿”å›å¹¶ä¸”å‘senderå‘é€ackã€‚è¿™æ˜¯æœ€å®¹æ˜“å®ç°çš„æ–¹å¼ï¼Œå› æ­¤è¢«å¹¿æ³›åº”ç”¨ã€‚ä½†æ˜¯ä¸ºäº†é¿å…æ­»é”ï¼Œå¿…é¡»å®ç°åœ¨å‘é€å’Œæ¥å—ç«¯éƒ½æœ‰å¾ˆå¤§bufferç©ºé—´çš„æƒ…å†µã€‚è¿™é‡Œsend()å‡½æ•°éœ€è¦ç­‰å¾…åŒ¹é…çš„recv()å‡½æ•°å‘é€ackï¼Œå› æ­¤senderå¯èƒ½ä¼šå‡ºç°aherad messagesæˆ–è€…æŒç»­å‘é€å¯¼è‡´buffer overflowç­‰æƒ…å†µé€ æˆæ­»é”ã€‚ä¸€ä¸ªè§£å†³æ–¹æ³•æ˜¯åˆ©ç”¨æ¨¡2æ“ä½œæ¥ç¡®å®šsend()å’Œrecv()çš„é¡ºåºï¼Œæ¯”å¦‚ä¸‹é¢è¿™æ®µä»£ç å°±å¯ä»¥é¿å…æ­»é”ã€‚

```c
void solve() {
	bool done = false;
	while (!done) {
		float my_diff = 0.0f;
		if (tid % 2 == 0) {
			sendDown(); recvDown();
			sendUp(); recvUp();
		} else {
			recvUp(); sendUp();
			recvDown(); sendDown();
		}
		â€¦
	}
}
```

**Non-blockling asynchronous send/recv**

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/asychsend.png" style="zoom:65%">

éé˜»å¡å‹é€šä¿¡å‡½æ•°ï¼Œsend()å’Œrecv()å‡ç«‹å³è¿”å›ï¼Œå¹¶ä¸”å‘é€ç«¯å’Œæ¥æ”¶ç«¯åœ¨ç­‰å¾…æ¶ˆæ¯å‘é€/æ¥å—æ—¶éƒ½å¯ä»¥è¿›è¡Œå…¶ä»–å·¥ä½œï¼Œåˆ©ç”¨checksend()å’Œcheckrecv()å‡½æ•°æ¥åˆ¤æ–­æ¶ˆæ¯æ˜¯å¦ä¼ é€’æˆåŠŸï¼Œå³æ˜¯å¦å¯ä»¥æ›´æ”¹bufferå†…å®¹ã€‚

#### Pipelining æµæ°´çº¿

~~è¯´åˆ°pipelingï¼Œå°±ä¸å¾—ä¸æƒ³åˆ°æˆ‘è€ƒè¯•å‰å¤ä¹ ç®—ä¸æ˜ç™½cyclesçœŸçš„å¤´å¤§è„‘å£³ç—›ã€‚ã€‚ã€‚æœ€åè¿˜æ˜¯æˆåŠŸçš„æ‰“äº†ä¸ªBåœ¨è¿™ä¸ªè¯¾ä¸Šï¼Œå·²ç»æ˜¯æˆ‘èƒ½åšçš„æœ€å¥½äº†ï¼ ğŸ¤¦â€â™‚ï¸ ä¸€å®šæ˜¯å› ä¸ºä¸æ˜åŸå› çš„å¤§è„‘å½“æœºï¼ä¸æ˜¯å› ä¸ºæˆ‘ç¬¨\_(:Ğ·ã€âˆ )_ æˆ‘å¾ˆåŠªåŠ›å¤ä¹ äº†ç¬¬äºŒä¸ªæœŸä¸­è€ƒè¯•å’Œç¬¬äºŒä¸ªç¬¬ä¸€ä¸ªæœŸä¸­è€ƒè¯•äº†ï¼~~

**Goal**: maximing throughtput of many loads

å…ˆå¤ä¹ ä¸€ä¸‹å»¶è¿Ÿå’Œå¸¦å®½ï¼š

* Latency å»¶è¿Ÿ: The amount of time needed for an operation to complete
* Bandwidth å¸¦å®½: The rate at which operations are performed

* Throughput ååé‡ï¼šthe rate of successful message delivery over a communication channel ([wiki](https://en.wikipedia.org/wiki/Throughput))

ä¸€ä¸ªæŒ‡ä»¤pipelineçš„ä¾‹å­å¦‚ä¸‹ï¼Œå¦‚å›¾æ‰€ç¤ºçš„latencyä¸º4 cycles/instruction, throughputä¸º1 instruction/cycleï¼š

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/laundry.png" style="zoom:65%">

Simple non-pipelined communication:

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/nonpipe.png" style="zoom:60%">

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/nonpipe2.png" style="zoom:59%">

Pipelined communication:

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/pipeline.png" style="zoom:55%">

#### Cost

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/cost.png" style="zoom:65%">

é€šä¿¡æˆæœ¬ = é€šä¿¡æ—¶é—´ - é‡å  

##### Two reason for communication

**Inherent communication**

Communication that must occur in a parallel algorithm, is fundamental given how the problem is decomposed and how work is assigned

**Communication to computation ratio è®¡ç®—é€šä¿¡æ¯”**

Definition: $$\frac{amount of communication (e.g. bytes)}{amount of computation (e.g. instructions)}$$

**Arithmetic intensity** è¿ç®—å¼ºåº¦: 1/communication to computation ratio

High arithmetic intensity (low communication-to-computation ratio) is required to efficiently utilize modern parallel processors since the ratio of compute capability to available bandwidth is high.

ä¸‰ç§ä¸åŒçš„ä»»åŠ¡åˆ†é…æ–¹å¼å¯¹åº”inherent communicationè®¡ç®—ï¼š

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/inherent.png" style="zoom:65%">

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/inherent1.png" style="zoom:60%">

**Artifactual communication**

All other communication results from practical details of system implementation, depends on machine implementation details

Examples:

* System might have a minimum granularity of transfer
* System might have rules of operation that result in unnecessary communication
* Poor placement of data in distributed memories
* Finite replication capacity

**Review of Cs**

* Cold miss: First time data touched. Unavoidable in a sequential program
* Capacity miss: Working set is larger than cache. Can be avoided/reduced by increasing cache size
* Conflict miss: Miss induced by cache management policy. Can be avoided/reduced by changing cache associativity, or data access pattern in application
* Communication miss: Due to inherent or artifactual communication in parallel system

~~ä¸çŸ¥é“è¿™å‡ ä¸ªçš„ä¸­æ–‡æ˜¯ä»€ä¹ˆï¼Œå¥½åƒåˆ†åˆ«æ˜¯å†·å¤±æ•ˆã€å®¹é‡å¤±æ•ˆã€å†²çªå¤±æ•ˆå’Œé€šä¿¡å¤±æ•ˆ~~

**Reducing communication costs**

* Reduce overhead of communication to sender/receiver
  * Send fewer messages, make messages larger (amortize overhead)

  * Coalesce many small messages into large ones

* Reduce delay
  * Application writer: restructure code to exploit locality

  * HW implementor: improve communication architecture

* Reduce contention

  * Replicate contended resources 

  * Stagger access to contended resources

* Increase communication/computation overlap
  * Application writer: use asynchronous communication 
  * HW implementor: pipelining, multi-threading, pre-fetching, out-of-order exec
  * Requires additional concurrency in application 

#### Techniques for reducing communication

 ç”¨è¿™ä¸ªgrid data accessæ¥ä¸¾ä¾‹ï¼š

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/griddataaccess.png" style="zoom:60%">

* Improved temporal locality by changing grid traversal order: "Blocked" iteration order

   <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/blocktravelgrid.png" style="zoom:58%">

* Improced temporal locality by fusing loops

  <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/fuseloop.png" style="zoom:62%">

* Improve arithmetic intensity by sharing data
  * Exploit sharing: co-locate tasks that operate on the same data
    * Reduces inherent communication
    * Schedule threads working on the same data structure at the same time on the same processor
  * Example: CUDA thread block

* Reducing artifactual communication: blocked data layout

  <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/blockdatagrid.png" style="zoom:60%">

##### Spatial locality ç©ºé—´å±€éƒ¨æ€§

**Granularity** ç²’åº¦ 

* Granularity of communication can be important because it may introduce artifactual communication
  * Granularity of communication / data transfer
  * Granularity of cache coherence

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/granularity.png" style="zoom:60%">

~~æ„Ÿè§‰ç°åœ¨çœ‹è¯¾ä»¶ä¹Ÿèƒ½çœ‹æ‡‚ï¼Œå½“æ—¶æ˜¯æ€ä¹ˆäº†å°±æ˜¯çœ‹ä¸æ‡‚å‘¢ï¼Ÿ~~

#### Contention ç«äº‰

Contention occurs when many requests to a resource are made within a small window of time

ä¸¾ä¸ªåœ¨å¹¶è¡Œæœºå™¨ä¸Šæ„å»ºç½‘æ ¼å¼æ•°æ®ç»“æ„çš„ä¾‹å­ï¼Œæ¯”å¦‚è®¡ç®—ç›¸é‚»interactionï¼š

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/computegrid.png" style="zoom:60%">

Solution 1: parallelize over cells

*  For each cell independently compute particles within it
  * Eliminates contention because no synchronization is required

* Insufficient parallelism: only 16 parallel tasks, but need thousands of independent tasks to efficiently utilize GPU

* Work inefficient: performs 16 times more particle-in-cell computations than sequential algorithm

```c
list cell_lists[16]; // 2D array of lists
for each cell c // in parallel
	for each particle p // sequentially
		if (p is within c)
			append p to cell_lists[c]
```

Solution 2: parallelize over particles

* Assign one particle to each CUDA thread. Thread computes cell containing particle, then atomically updates list
  * Massive contention: thousands of threads contending for access to update single shared data structure

```c
list cell_list[16]; // 2D array of lists
lock cell_list_lock;
for each particle p // in parallel
	c = compute cell containing p
	lock(cell_list_lock)
	append p to cell_list[c]
	unlock(cell_list_lock)
```

Solution 3: use finer-granularity locks

* Alleviate contention for single global lock by using per-cell locks
  * 16x less contention than solution 2, assuming uniform distribution of particles in 2D space

```c
list cell_list[16]; // 2D array of lists
lock cell_list_lock[16];
for each particle p // in parallel
	c = compute cell containing p
	lock(cell_list_lock[c])
	append p to cell_list[c]
	unlock(cell_list_lock[c])
```

Solution 4: compute partial results + merge

* Generate N â€œpartialâ€ grids in parallel, then combine
  * Requires extra work: merging the N grids at the end of the computation
  * Requires extra memory footprint: Store N grids of lists, rather than 1

Solution 5: data-parallel approach

* Step 1: compute cell containing each particle (parallel over input particles)
* Step 2: sort results by cell (particle index array permuted based on sort)
* Step 3: find start/end of each cell (parallel over particle_index elements)

#### Improving program performance

* Identify and exploit locality: communicate less (increase arithmetic intensity)

* Reduce overhead (fewer, large messages)
* Reduce contention
* Maximize overlap of communication and processing (hide latency so as to not incur cost)

### Reference

[å¹¶è¡Œè®¡ç®—å¯¼è®º](http://staff.ustc.edu.cn/~wzhao7/c_index_files/main.files/parrel.pdf)

[å¹¶è¡Œè®¡ç®—å…¥é—¨blogä¸€ç¯‡](http://blog.leanote.com/post/454858191@qq.com/ffed2c2cbfb2#title-59)

~~æˆ‘éƒ½æ¯•ä¸šåŠå¹´å¤šäº†å±…ç„¶è¿˜åœ¨çœ‹è¯¾ä»¶å’Œè§†é¢‘orz è¿™æ¯«æ— æ„ä¹‰ï¼Œä¸å¦‚åˆ·é¢˜ã€‚ä½†æ˜¯äººæ€»æ˜¯é€šè¿‡åšä¸€äº›ä¸é‡è¦çš„äº‹æ¥é¿å…åšé‚£äº›é‡è¦çš„äº‹ï¼Œé€šè¿‡åšåˆ«çš„äº‹æ¥ç¼“è§£ä¸åšè¯¥åšçš„äº‹çš„ç„¦è™‘ï¼Œæ‰€ä»¥æˆ‘è¿˜åœ¨å†™ï¼Œå°±å¦‚å½“æ—¶åœ¨æ™šä¸Šåšé²œèŠ‹ä»™ä¸€æ ·ã€‚åæ­£ä¹Ÿæ˜¯è‡ªå·±çš„åšå®¢ä¸§ä¸€ä¸§æ²¡å…³ç³»å§ï¼Œå¤´å¤§è„‘å£³ç—›ã€‚~~

