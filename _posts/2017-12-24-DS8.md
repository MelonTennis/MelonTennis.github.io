---
layout: post
title: Distributed System VIII
subtitle: 15640 DS8 - GFS/HDFS/Spanner
date: 2017-12-24
categories: Note
tags: [DS, 15640]
catalog: true
---

## Distributed System VIII

ç°åœ¨PITé£å¾€DENçš„âœˆï¸ã€‚DanielğŸ„è¦é£å›ğŸ‡©ğŸ‡ªï¼Œè§‰å¾—å¶é‡æœºä¼šä¸º0ï¼Œæœç„¶æœºåœºæ²¡é‡åˆ°Danielå¥½å¯æƒœå•Šã€‚è¿™ä¸€ç¯‡ä¸»è¦å…³äºè°·æ­Œå’ŒHadoopçš„æ–‡ä»¶ç³»ç»Ÿæ¡ˆä¾‹å­¦ä¹ ï¼ŒGFS/HDFSã€‚ä»¥åŠè°·æ­ŒDatabaseï¼ŒSpannerã€‚ä»æ¥ä¸æ™•æœºçš„æˆ‘å¥½åƒæœ‰ç‚¹æ™•æœºï¼Œç³Ÿç³•(;Â´à¼àº¶Ğ”à¼àº¶`)

### GFS/HDFS

Google file systemå’ŒHadoop file systemçš„ç»“æ„å¾ˆç›¸ä¼¼ï¼Œéƒ½æ˜¯å°†æ–‡ä»¶åˆ†æˆblocksåˆ†åˆ«å­˜åœ¨chunkç»“æ„ä¸­ï¼Œæ ¹æ®masterä¸­å­˜å‚¨çš„metadataæ¥è¿›è¡Œè®¿é—®ã€‚

#### GFS

GFS is a distributed fault-tolerant file system.

Assumption: large files/large sequential writes and append/large streaming reads/concurrent appends by multiple clients

GFSçš„é‡ç‚¹åœ¨äºç†è§£GFSçš„ç»“æ„å’Œè¯»å†™æ“ä½œæ¨¡å‹ï¼Œå…¶ä»–çš„å€’ä¹Ÿæ²¡ä»€ä¹ˆ...

**GFS Architecture**

* One master server
* Many chunk servers: 64MB portion of file, identified by global ids
* Many clients accessing same/different files stored on same cluster

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/gfs.png" style="zoom:45%">

GFSçš„master serverå­˜å‚¨æ‰€æœ‰çš„metadataï¼Œè´Ÿè´£ä¸Clienté€šä¿¡å¹¶ä¸”ä¿è¯ç³»ç»Ÿçš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶é€šè¿‡ä¸Chunk serversé€šä¿¡æ¥è¿ç§»æ•°æ®ã€‚Master serverå°†æ‰€æœ‰çš„metadataå­˜åœ¨RAMä¸­ï¼Œå› æ­¤å…·æœ‰å¾ˆå¿«çš„é€Ÿåº¦ã€‚Chunk serverå¯¹äºdataæ²¡æœ‰æ•´ä½“æ€§çš„è®¤çŸ¥ï¼Œé€šè¿‡ä¸master severé€šä¿¡æ¥ä¼ é€’ï¼æ›´æ–°æ•°æ®ã€‚Chunk serverä¸ç¼“å­˜ä»»ä½•æ•°æ®ï¼Œæ•°æ®å­˜åœ¨ç¡¬ç›˜çš„blockä¸­ï¼Œå®šæœŸå‘master serverå‘é€heartbeatã€‚Clientç¼“å­˜metadataï¼Œä¸ç¼“å­˜ä»»ä½•æ•°æ®ï¼Œé€šè¿‡ä¸master serverè¿›è¡ŒW/Ræ“ä½œè®¿é—®æ•°æ®ã€‚

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/gfs2.png" style="zoom:45%">

##### GFS Client Read

* Client sends master: read(file name, chunk index)
* Masterâ€™s reply: chunk ID, chunk version number, locations of replicas
* Client sends â€œclosestâ€ chunkserver with replica:
  * read (chunk ID, byte range)
  * â€œClosestâ€ determined by IP address on simple rack based network topology
* Chunkserver replies with data

##### GFS Client Write

* All replicas acknowledge data write to client
* Client sends write request to primary (commit phase)
* Primary assigns serial number to write request, providing ordering
* Primary forwards write request with same serial number to secondary replicas
* Secondary replicas all reply to primary after completing writes in the same order
* Primary replies to client

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/gfswrite.png" style="zoom:45%">

GFSçš„å†™æ“ä½œï¼Œæ•°æ®ä»Clientæµå‘æœ€è¿‘çš„replicaï¼Œç›´åˆ°æ‰€æœ‰çš„replicaéƒ½å¾—åˆ°äº†æ›´æ–°çš„æ•°æ®ã€‚Replicaå¾—åˆ°äº†æ›´æ–°çš„æ•°æ®åä¼šä¸primaryé€šä¿¡ã€‚PrimaryçŸ¥é“æ‰€æœ‰çš„replicaéƒ½åˆ°äº†æ›´æ–°ä¼šé€šçŸ¥Clientã€‚

##### GFS Consistency Model

* Changes to data are primary ordered as chosen by a primary
* Record append completes **at least once**, at offset of GFS's choosing
* Failures can cause inconsistency

##### GFS Limitations

* Master biggest impediment to scaling
  * Performance bottleneck
  * Holds all data structures in memory
  * Takes long time to rebuild metadata
  * Must vulnerable point for reliability
  * Solution:
    * Have systems with multiple master nodes, all sharing set of chunk servers.
    * Not a uniform name space.
* Large chunk size
  * Canâ€™t afford to make smaller, since this would create more work for master. 

##### GFS Summary

* Success: used actively by Google to support search service and other applications
* Semantics not transparent to apps
  * Must verify file contents to avoid inconsistent regions, repeated appends (at least once)
* Performance not good for all apps
  * Assumes read once, write once workload (no client caching!)

#### HDFS

Hadoop file system, å’ŒGFSéå¸¸ç›¸ä¼¼ã€‚å°†master serveræ¢æˆname nodeï¼Œchunk serveræ¢åšdata noteï¼Œchunkå¤§å°ä¸æ˜¯64MBè€Œæ˜¯128MBã€‚ç»“æ„å‡ ä¹ä¸€è‡´ã€‚~~æ²¡æœ‰ä»€ä¹ˆé‡ç‚¹çš„æ ·å­ã€‚~~

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/hdfs.png" style="zoom:50%">

##### GFS Vs HDFS

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/gfsVShdfs.png" style="zoom:50%">

 ### Spanner

...å››å¤©å äº‹å®è¯æ˜æˆ‘çš„è¯å‡æœŸæœç„¶å¾ˆéš¾è¿›è¡Œå­¦æœ¯æ´»åŠ¨\_(:Ğ·ã€âˆ )\_

Spanner,è°·æ­Œçš„åˆ†å¸ƒå¼æ•°æ®åº“ï¼Œç„¶è€Œè°·æ­Œä¸ç»™æˆ‘é¢è¯•ï¼Œä¸æƒ³å†™Spannerï¼Œæ°”æ„¤ã€‚

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/spanner.png" style="zoom:50%">

æ•°æ®åº“å˜›ï¼ŒSQLè¯­è¨€ï¼ŒACIDä»€ä¹ˆçš„ã€‚å¯¹äºåˆ†å¸ƒå¼çš„äº‹åŠ¡å…·æœ‰å¤–éƒ¨ä¸€è‡´æ€§çš„ç‰¹å¾ï¼Œåˆ©ç”¨2PCå®ç°ã€‚Fault-toleranceå’ŒreplicaåŸºäºPaxoså®ç°ã€‚Spannerçš„ä¸­å¿ƒæ€æƒ³æ˜¯å¯¹äºæ¯ä¸ªTransactionä¿å­˜ä¸€ä¸ªæ—¶é—´æ®µ(ä½œä¸ºTimestamp)ï¼Œæ ¹æ®ä¸¤ä¸ªæ—¶é—´æ®µæ˜¯å¦é‡å æ¥åˆ¤æ–­ä¸¤ä¸ªäº‹åŠ¡çš„å‘ç”Ÿæ¬¡åº, Timetampçš„é¡ºåºå°±æ˜¯commitçš„é¡ºåºã€‚~~æˆ‘å·²ç»å…¨å¿˜äº†~~

#### Spanner Concurrency Control

Key aspect of differentiating Spanner â€“ using globally meaningful timestamps for distributed transactions in achieving external consistency

å¯¹äºæ¯ä¸ªTransactionï¼Œå½“å¤©æ—¶é—´åˆ†åˆ«+-æ—¶é—´æ®µÎµ(å¥½åƒä¸€èˆ¬æ˜¯ä¼ è¾“æ—¶é—´æ¥ç€)ï¼Œæ„æˆæ—¶é—´æ®µã€‚

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/spannertime.png" style="zoom:50%">

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/ttapi.png" style="zoom:50%">

ä¿è¯å¤–éƒ¨ä¸€è‡´æ€§ä¹Ÿå°±æ˜¯ï¼Œå¦‚æœT1äº‹åŠ¡çš„ç»“æŸæ¯”T2äº‹åŠ¡çš„å¼€å§‹æ—©ï¼Œå¯ä»¥è®¤ä¸ºT1çš„timestampå°äºT2.



å‚è€ƒèµ„æ–™:

[Google Filesystem](http://research.google.com/archive/gfs-sosp2003.pdf)

[GFS followup interview](http://queue.acm.org/detail.cfm?id=1594206)

[Spanner paper](https://www.usenix.org/system/files/conference/osdi12/osdi12-final-16.pdf)

[Consistent hashing paper](http://cs.brown.edu/courses/csci2950-u/papers/chash97stoc.pdf)