---
layout: post
title: Deep Learning I
subtitle: 11785 DL1 - NN Training & Optimization
date: 2018-02-04
categories: Note
tags: [DL]
catalog: true
---

## Deep Learning I

å½“ç„¶é¦–å…ˆæ˜¯å…³äºç¥ç»ç½‘ç»œçš„åŸºç¡€çŸ¥è¯†å’Œè®­ç»ƒç®—æ³•ã€‚

æ·±åº¦å­¦ä¹ å¾ˆç«ï¼Œå­¦å­¦çœ‹çœ‹ã€‚ä¹Ÿä¸æ˜¯è¯´ä»€ä¹ˆç«å°±è¦å­¦ä»€ä¹ˆã€‚çå­¦è¥¿è¥¿â•®(â•¯â–½â•°)â•­

~~å¯æˆ‘è¿˜æ˜¯æƒ³å­¦æ•°æ®åº“ã€‚æ‰§å¿µã€‚è¿™å­¦æœŸçš„é«˜çº§æ•°æ®åº“ä¸é€‚åˆæˆ‘ã€‚å“­å”§å”§ã€‚~~

ä½†æ˜¯ä¸ºäº†åŠ æ·±å°è±¡è¿˜æ˜¯è¦å†™ç¬”è®°ã€‚

é¦–å…ˆä»‹ç»äº†èƒŒæ™¯çŸ¥è¯†ï¼ŒConnectionist Machinesï¼Œ A-type, B-type machines, brain models, Hebbian learning, Rosenblattâ€™s perceptronä»€ä¹ˆçš„ã€‚è§‰å¾—è¿™äº›äº†è§£ä¸€ä¸‹å°±å¥½ï¼Œperceptronå­¦è¿‡å¥½å‡ æ¬¡ï¼Œæ˜¯å¾ˆé‡è¦çš„æ¦‚å¿µã€‚

### Perceptron æ„ŸçŸ¥æœº

æœºå™¨å­¦ä¹ çš„ç»å…¸æ¨¡å‹æ„ŸçŸ¥æœºï¼Œä¸€ç§äºŒå…ƒåˆ†ç±»å™¨ï¼Œå°†è¾“å…¥å‘é‡**x**æ˜ å°„åˆ°äºŒå…ƒè¾“å‡ºå€¼yä¸Šã€‚

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/perceptron.png" style="zoom:50%">

Sequential learningï¼š

* Update the weights whenever the perceptron output is wrong
* Proved convergence

äºŒå…ƒæ„ŸçŸ¥æœºå¯ä»¥çœ‹çº¿æ€§åˆ†ç±»å™¨ï¼Œå¯ä»¥è¡¨ç¤ºæ‰€æœ‰é™¤äº†XORå¤–çš„booleanæ“ä½œã€‚

Activation æ¿€æ´»å‡½æ•°: The function that acts on the weighted combination of inputs (and threshold) 

ä¸€èˆ¬æ˜¯sigmodï¼Ÿç¬¬ä¸€æ¬¡ä½œä¸šå®ç°äº†sigmodï¼Œtanhï¼Œreluã€‚ä½œä¸ºæ¿€æ´»å‡½æ•°ï¼Œè¦æ±‚æ˜¯å¯å¯¼å¹¶ä¸”éçº¿æ€§ã€‚				

### MLP 

å¤šå±‚æ„ŸçŸ¥æœºï¼Œå¯ä»¥å°†è¾“å…¥å‘é‡æ˜ å°„åˆ°è¾“å‡ºå‘é‡ã€‚å…‹æœäº†å•ä¸ªæ„ŸçŸ¥æœºæ— æ³•è¯†åˆ«çº¿æ€§ä¸å¯åˆ†æ•°æ®çš„å¼±ç‚¹ã€‚

* MLPs are connectionist computational models
  * Individual perceptrons are computational equivalent of neurons
  * MLP is a layered composition of many perceptrons
* MLPs can model any Boolean functions
  * Individual perceptrons can act as Boolean gates
  * Networks of perceptrons are Boolean functions
* MLPs are Boolean machines
  * They can represent arbitrary decision boundaries
  * They can be used to classify data
* MLP can also model continuous valued functions
* MLPs are classification engines
* MLP can also model continuous valued functions

Backgroundä»‹ç»å®Œåå°±è¿›å…¥deepå­¦ä¹ ï¼Œdepth > 2 ç®—ä½œdeepã€‚

ç”±äºboolean functionå¯ä»¥çœ‹ä½œçœŸå€¼è¡¨çš„é›†åˆï¼Œæ‰€ä»¥ç”¨å•éšå±‚çš„MLPå°±å¯ä»¥è¡¨ç¤ºæ‰€æœ‰çš„boolean functionã€‚

å¯¹äºsingle-layer boolean network:

* 2^N-1 perceptrons in hidden layer - exponential
* O(N2^N-1) weights - superexponential

åŒæ ·çš„boolean functionï¼Œä»¥æ‰€æœ‰è¾“å…¥çš„XORä¸ºä¾‹å­ï¼š

* 3N-1 perceptrons in deep network
* 9(N-1) parameters - linear
* can be arranged in 2log2(N) layers

å¾—å‡ºç»“è®ºï¼š

- Reducing the number of layers below the minimum will result in anexponentially sized network to express the function fully
- A network with fewer than the minimum required number of neurons cannot model the function
- Any Boolean circuit of depth *d* using AND, OR and NOT gates with unbounded fan-in must have size 2^(n^1/d)

Shannonâ€™s theorem:  For n > 2, there is Boolean function of variables that requires at least 2^n/n gates

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/deep1.png" style="zoom:50%">

* For general case of mutually intersecting hyperplanes in *D* dimensions, we will need O(N^D/(D-1)!) weights (assuming N >> D)
  * Increasing input dimensions can increase the worst-case size of the shallower network exponentially, but not the XOR net

MLPä¹Ÿå¯ä»¥å¯¹è¿ç»­å€¼è¿›è¡Œå›å½’ï¼š

* An MLP with many units can model an arbitrary function over an input
* A one-layer MLP can model an arbitrary function of a single input
* MLPs with additive output units are universal approximators
* A neural network can represent any function provided it has sufficient capacity
  * Not all architectures can represent any function

ä¸€ä¸ªè¯¾ä»¶ä¸€ç™¾å¤šé¡µpptä¸ºä»€ä¹ˆæˆ‘å†™ç¬”è®°å´åªæœ‰å‡ è¡Œ(._.)

### Neural Network

Architecture: MLPs

Parameters: weights and bias

Learning: determine value of parameters so network computes desired function

 <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/nn1.png" style="zoom:50%">

* More generally, given the function to model, we can derive the parameters of the network to model it, through computation

ä¸è¿‡ä¸€èˆ¬æƒ…å†µä¸‹g(X)æ˜¯ä¸çŸ¥é“çš„ï¼šSampling g(x)

* Basically,get input-output pairs for a number of samples of input , many samples (ğ‘‹ , ğ‘‘ ), where ğ‘‘  = ğ‘”(ğ‘‹)  + ğ‘›ğ‘œğ‘–ğ‘ ğ‘’
* Good sampling: the samples of *X* will be drawn from *P(X)*

Learning function: estimate the network parameters to â€œfitâ€ the training points exactly

**Perceptron Learning Algorithm**

| Perceptron learning algo                 | Summary                                  |
| ---------------------------------------- | ---------------------------------------- |
| <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/pl.png" style="zoom:50%"> | <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/pl2.png" style="zoom:40%"> |

* Guaranteed to converge if classes are linearly separable
* After no more than (R/r)^2 misclassifications

#### Greddy algorithms

**ADALINE**: Adaptive Linear Neuron

| Learning                                 | Rule                                     |
| ---------------------------------------- | ---------------------------------------- |
| <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/adaline1.png" style="zoom:40%"> | <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/adaline2.png" style="zoom:40%"> |
| <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/adaline3.png" style="zoom:40%"> | <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/adaline4.png" style="zoom:40%"> |

**MADLINE**: Multiple adaline

* Not very useful for large nets, effective for small networks - too expensive/greedy

| Training                                 | Learning                                 |
| ---------------------------------------- | ---------------------------------------- |
| <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/madline1.png" style="zoom:40%"> | <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/madline2.png" style="zoom:40%"> |

#### Risk & Error

**Empirical risk**

* The empirical risk is only an empirical approximation to the true risk which is our actual minimization objective 

| Risk                                     | Minimization                             |
| ---------------------------------------- | ---------------------------------------- |
| <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/re1.png" style="zoom:40%"> | <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/re2.png" style="zoom:40%"> |

æ±‚ä½¿Erræœ€å°çš„Wå€¼ï¼ŒçŸ©é˜µæ±‚å¯¼çŸ¥è¯†ï¼Œç”±äºå¤±å¿†ç¬¬ä¸€ä¸ªä½œä¸šç–¯ç‹‚æ¨å¯¼å¥½ä¹…ã€‚

Problem statementï¼š

| Find min                                 | Hession                                  |
| ---------------------------------------- | ---------------------------------------- |
| <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/gad.png" style="zoom:40%"> | <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/mn.png" style="zoom:40%"> |

**Solutions**

| Closed Form                              | Iterative                                |
| ---------------------------------------- | ---------------------------------------- |
| <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/cf.png" style="zoom:40%"> | <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/it.png" style="zoom:40%"> |

å› ä¸ºClosed formä¸ä¸€å®šå¯è§£ï¼Œå¼•å…¥äº†è¿­ä»£æ±‚è§£çš„æ–¹æ³•ï¼Œå€Ÿæ­¤ä»‹ç»éå¸¸å¸¸ç”¨çš„trainingç®—æ³•æ¢¯åº¦ä¸‹é™ã€‚

### Gradient Descent

| Approach                                 | Multivariate                             |
| ---------------------------------------- | ---------------------------------------- |
| <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/gd1.png" style="zoom:40%"> | <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/gd2.png" style="zoom:40%"> |
| <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/gd3.png" style="zoom:40%"> | <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/gd4.png" style="zoom:40%"> |

æ‰€ä»¥ç°åœ¨çš„é—®é¢˜å˜æˆå¦‚ä½•é€šè¿‡GDæ¥æ±‚è§£æœ€ä¼˜çš„å‚æ•°ä½¿Objective(Err/Cost)è¾¾åˆ°æœ€å°ã€‚

Problem statemet:

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/prb.png" style="zoom:50%">

å…¶ä¸­(***X***, ***d***)æ˜¯input, ç”¨çŸ©é˜µå½¢å¼è¡¨ç¤º, *W*æ˜¯ç¥ç»ç½‘ç»œçš„parametersï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬è¦å¾—åˆ°çš„è®­ç»ƒç»“æœ, *div*è®¡ç®—é€šè¿‡ç¥ç»ç½‘ç»œpredictçš„å€¼ä¸çœŸå€¼çš„err, ä¹Ÿå°±æ˜¯objectiveç›®æ ‡æ–¹ç¨‹ï¼Œğ‘“è¡¨ç¤ºè¿™ä¸ªç½‘ç»œã€‚

**Network Construction**

| Individual Neuron                        | Activations                              |
| ---------------------------------------- | ---------------------------------------- |
| <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/tnn1.png" style="zoom:40%"> | <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/tnn2.png" style="zoom:40%"> |
| Combination                              | Notation                                 |
| <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/tnn3.png" style="zoom:40%"> | <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/tnn4.png" style="zoom:40%"> |

**Training by GD**

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/tgd.png" style="zoom:50%">

Objectiveå¯¹Wçš„å¯¼æ•°ï¼Œæ˜¯Objectiveå¯¹Wijå¯¼æ•°çš„çŸ©é˜µè¡¨è¾¾å½¢å¼ã€‚åˆ©ç”¨chain ruleä»æœ€åä¸€å±‚ä¸€å±‚ä¸€å±‚å‘å‰æ±‚å¯¼ï¼Œç›´åˆ°å¾—åˆ°ç»“æœã€‚

| Chain rule                               | Network                                  |
| ---------------------------------------- | ---------------------------------------- |
| <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/cr1.png" style="zoom:40%"> | <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/cr2.png" style="zoom:40%"> |

ç”¨æ¢¯åº¦ä¸‹é™æ¥è®­ç»ƒç¥ç»ç½‘ç»œï¼Œé¦–å…ˆæ ¹æ®åˆå§‹åŒ–çš„weightså’Œbiasè®¡ç®—å‡ºpredictçš„å€¼ï¼Œæ ¹æ®objective functionå’Œground truthå¾—åˆ°loss/err. ä¸ºäº†å¾—åˆ°ä½¿è¿™ä¸ªobjectiveæœ€å°çš„weightså’Œbiasåˆ©ç”¨æ¢¯åº¦ä¸‹é™æ¥æ±‚è§£ã€‚å› æ­¤éœ€è¦backward computationï¼Œå¯¹æ¯ä¸€å±‚çš„weightså’Œbiasæ±‚å‡ºobjectiveå¯¹å®ƒä»¬çš„åå¯¼ï¼Œæ ¹æ®æ›´æ–°è§„åˆ™æ›´æ–°weightsã€‚ä¸æ–­é‡å¤è¿™ä¸ªè¿‡ç¨‹ç›´åˆ°æ”¶æ•›ã€‚

| Forward pass                             | Backward pass                            |
| ---------------------------------------- | ---------------------------------------- |
| <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/fdp.png" style="zoom:40%"> | <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/bdp.png" style="zoom:40%"> |

**Training by BackProp**

| Sudo code                                | Vector formulation                       |
| ---------------------------------------- | ---------------------------------------- |
| <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/bp.png" style="zoom:40%"> | <img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/bpv.png" style="zoom:40%"> |

* Assumptions: all these conditions are frequently not applicable
  * The computation of the output of one neuron does not directly affect
    computation of other neurons in the same (or previous) layers
  * Outputs of neurons only combine through weighted addition 
  * Activations are actually differentiable
* Vector activations: all outputs are functions of all inputs

å‰ä¸¤å‘¨çš„å†…å®¹åŸºæœ¬éƒ½è¢«ä¹‹å‰çš„601å’Œ605è¦†ç›–äº†ï¼Œä¸è¿‡æ—¶é—´ä¹…äº†ä¹Ÿæœ‰äº›å¤±å¿†ï¼Œå°±å½“ä½œå¤ä¹ ä¸€ä¸‹ã€‚

### å‚è€ƒèµ„æ–™

[Multilayer Feedforward NEtworks](https://pdfs.semanticscholar.org/f22f/6972e66bdd2e769fa64b0df0a13063c0c101.pdf)	

[Deep Sum-Product Networks](https://www.researchgate.net/publication/236736771_Shallow_vs_Deep_Sum-Product_Networks)

[VC Dimension of NN](http://mathsci.kaist.ac.kr/~nipl/mas557/VCD_ANN_3.pdf)

[A Capacity Scaling Law for ANN](https://arxiv.org/pdf/1708.06019.pdf)

[30 Years of ANN](https://51d52ae2-a-62cb3a1a-s-sites.googlegroups.com/site/handoffnn/neural-network/can-kao-deneural-network-zi-liao/NN-30å¹´ç™¼å±•.pdf?attachauth=ANoY7cq5XxLKsckxplkqgACkuTcu9kaOYpWQnQbOZypoNMyakHjPutq9fnibNOLeVHrqhwqNZaeGzQRWLIzt1vazuqvNktw4LmRfzBo7kfaY3JFzy2HOwolZiMIBMGzheMv7Hxsl1Lwqqn0YQvGrG7ikyEyrqryLS-un5oBNOR5OPgWIKkQwC-cEiLCzBkBBM9tgcVjndjnNO5ufQXxLhSswn_JnAlhQJZ3yxpdiB_8w9PuVCA25KJkI_fl3xTAjvz088_G2Mzv-5lh9kiRQazb4KwsxbIZkadClLRptlMboq9X2fHclRQ1rHV8jlZt4ue1XI284JLLq&attredirects=0)

[Perceptron Mistake Bounds](https://arxiv.org/pdf/1305.0208.pdf)

[Learning representations by BP err](https://www.nature.com/articles/323533a0)