---
dzzlayout: post
title: Parallel Architecture & Programing VIII
subtitle: 15618 PP8 Synchronization Implementing
date: 2019-02-01
categories: Note
tags: [PP]
catalog: true
header-img: "img/ctc1.jpeg"
---

## Parallel Architecture & Programing VIII

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

~~ğŸ‹ğŸ‹ğŸ‹åŒ…å›´ç€æˆ‘ï¼Œä»Šå¤©ä¹Ÿè¦åŠ æ²¹é¸­ã€‚WFHäº†æ‰€ä»¥æ²¡æœ‰è‡ªé—­ï¼Œç”šè‡³æ²¡æœ‰å·¥ä½œï¼Œè€Œæ˜¯åœ¨å­¦ä¹ å¦‚ä½•å®ç°åŒæ­¥ï¼Œè™½ç„¶æ²¡å•¥ç”¨ã€‚å•Šå•Šå•Šå•Šå•Šå•Šå•Šå†…å¿ƒååˆ†éš¾å—äº†ã€‚å•Šéš¾å—\_(:Ğ·ã€âˆ )_æˆ‘æ˜¯æƒ³å¥½å¥½å·¥ä½œçš„~~

### Implementing Synchronization 

**Primitives for ensuring mutual exclusion**

* Locks
* Atomic primitives
* Transactions

**Primitives for event signaling**

* Barriers
* Flags

**Three phases of a synchronization event**

* Acquire method
* Waiting algorithm
  * Busy waiting (spinning)
    * Scheduling overhead is larger than expected wait time
    * Processorâ€™s resources not needed for other tasks
  * Blocking: if progress cannot be made because a resource cannot be acquired, it is desirable to free up execution resources for another thread
* Release method

#### Implemenging Locks

**Desirable lock performance**

* Low latency: If lock is free and no other processors are trying to acquire it, a processor should

  be able to acquire the lock quickly

* Low interconnect traffic: If all processors are trying to acquire lock at once, they should acquire the lock in succession with as little traffic as possible
* Scalability: Latency / traffic should scale reasonably with number of processors
* Low storage cost
* Fairness: Avoid starvation or substantial unfairness

**Test and set lock**

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/testandset.png" style="zoom:60%">

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/testandset1.png" style="zoom:62%">

Characteristicsï¼š

*  Slightly higher latency than test-and-set in uncontended case
* Generates much less interconnect traffic
  * One invalidation, per waiting processor, per lock release (O(P) invalidations)
  * O(P2) interconnect traffic if all processors have the lock cached
  * Test-and-set lock generated one invalidation per waiting processor per test
* More scalable (due to less traffic)
* Storage cost unchanged (one int)
* Still no provisions for fairness

Test-and-set lock with back offï¼š

* Upon failure to acquire lock, delay for awhile before retrying

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/testandset2.png" style="zoom:68%">

* Same uncontended latency as test-and-set, but potentially higher latency under contention
* Generates less traffic than test-and-set
* Improves scalability (due to less traffic)
* Storage cost unchanged
* Exponential back-off can cause severe unfairness (newer requesters back off for shorter intervals)
* Main problem with test-and-set style locks: upon release, all waiting processors attempt to acquire lock using test-and-set

~~æƒ³å›å·¥å¤§åƒå¹´å¤œé¥­~~ Test and set lockä¿è¯å…±äº«èµ„æºåœ¨ä»»ä¸€æ—¶åˆ»åªæœ‰ä¸€ä¸ªé”çš„æ‹¥æœ‰è€…ï¼Œå…¶ä»–å¤„ç†å™¨busy waiting. 

**Ticket lock**

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/ticketlock.png" style="zoom:65%">

* No atomic operation needed to acquire the lock (only a read)
* Only one invalidation per lock release (O(P) interconnect traffic)

**Array-based lock**

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/arraybaselock.png" style="zoom:65%">

* Each processor spins on a different memory address 
* Utilizes atomic operation to assign address on attempt to acquire

* O(1) interconnect traffic per release, but lock requires space linear in P
* The atomic circular increment is a more complex operation (higher overhead)

 **Queue-based Lock (MCS lock)**

* Create a queue of waiters
  * Each thread allocates a local space on which to wait

* Pseudo-code:
  * Glock â€“ global lock
  * Mlock â€“my lock (state, next pointer)

ã€msclockã€‘

#### Implementing Barriers

**Centralized barrier**

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/centralbarrier.png" style="zoom:65%">

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/centralbarrier1.png" style="zoom:65%">

* O(P) traffic on interconnect per barrierï¼š
  * All threads: 2P write transactions to obtain barrier lock and update counter
  * Last thread: 2 write transactions to write to the flag and reset the counter

* Still serialization on a single shared lock
  * So span (latency) of entire operation is O(P)
  * Optimizationï¼šTree implementation

**Tree implementation of barrier**

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/treebarrier.png" style="zoom:65%">

* Combining trees make better use of parallelism in interconnect topologies
  * lg(P) span (latency)
  * Strategy makes less sense on a bus (all traffic still serialized on single shared bus)

* Barrier acquire: when processor arrives at barrier, performs increment of parent counter
* Barrier release: beginning from root, notify children of release

