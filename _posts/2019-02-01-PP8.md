---
dzzlayout: post
title: Parallel Architecture & Programing VIII
subtitle: 15618 PP8 Synchronization Implementing
date: 2019-02-01
categories: Note
tags: [PP]
catalog: true
header-img: "img/ctc1.jpeg"
---

## Parallel Architecture & Programing VIII

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

~~🍋🍋🍋包围着我，今天也要加油鸭。WFH了所以没有自闭，甚至没有工作，而是在学习如何实现同步，虽然没啥用。啊啊啊啊啊啊啊内心十分难受了。啊难受\_(:з」∠)_我是想好好工作的~~

### Implementing Synchronization 

**Primitives for ensuring mutual exclusion**

* Locks
* Atomic primitives
* Transactions

**Primitives for event signaling**

* Barriers
* Flags

**Three phases of a synchronization event**

* Acquire method
* Waiting algorithm
  * Busy waiting (spinning)
    * Scheduling overhead is larger than expected wait time
    * Processor’s resources not needed for other tasks
  * Blocking: if progress cannot be made because a resource cannot be acquired, it is desirable to free up execution resources for another thread
* Release method

#### Implemenging Locks

**Desirable lock performance**

* Low latency: If lock is free and no other processors are trying to acquire it, a processor should

  be able to acquire the lock quickly

* Low interconnect traffic: If all processors are trying to acquire lock at once, they should acquire the lock in succession with as little traffic as possible
* Scalability: Latency / traffic should scale reasonably with number of processors
* Low storage cost
* Fairness: Avoid starvation or substantial unfairness

**Test and set lock**

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/testandset.png" style="zoom:60%">

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/testandset1.png" style="zoom:62%">

Characteristics：

*  Slightly higher latency than test-and-set in uncontended case
* Generates much less interconnect traffic
  * One invalidation, per waiting processor, per lock release (O(P) invalidations)
  * O(P2) interconnect traffic if all processors have the lock cached
  * Test-and-set lock generated one invalidation per waiting processor per test
* More scalable (due to less traffic)
* Storage cost unchanged (one int)
* Still no provisions for fairness

Test-and-set lock with back off：

* Upon failure to acquire lock, delay for awhile before retrying

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/testandset2.png" style="zoom:68%">

* Same uncontended latency as test-and-set, but potentially higher latency under contention
* Generates less traffic than test-and-set
* Improves scalability (due to less traffic)
* Storage cost unchanged
* Exponential back-off can cause severe unfairness (newer requesters back off for shorter intervals)
* Main problem with test-and-set style locks: upon release, all waiting processors attempt to acquire lock using test-and-set

~~想回工大吃年夜饭~~ Test and set lock保证共享资源在任一时刻只有一个锁的拥有者，其他处理器busy waiting. 

**Ticket lock**

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/ticketlock.png" style="zoom:65%">

* No atomic operation needed to acquire the lock (only a read)
* Only one invalidation per lock release (O(P) interconnect traffic)

**Array-based lock**

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/arraybaselock.png" style="zoom:65%">

* Each processor spins on a different memory address 
* Utilizes atomic operation to assign address on attempt to acquire

* O(1) interconnect traffic per release, but lock requires space linear in P
* The atomic circular increment is a more complex operation (higher overhead)

 **Queue-based Lock (MCS lock)**

* Create a queue of waiters
  * Each thread allocates a local space on which to wait

* Pseudo-code:
  * Glock – global lock
  * Mlock –my lock (state, next pointer)

【msclock】

#### Implementing Barriers

**Centralized barrier**

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/centralbarrier.png" style="zoom:65%">

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/centralbarrier1.png" style="zoom:65%">

* O(P) traffic on interconnect per barrier：
  * All threads: 2P write transactions to obtain barrier lock and update counter
  * Last thread: 2 write transactions to write to the flag and reset the counter

* Still serialization on a single shared lock
  * So span (latency) of entire operation is O(P)
  * Optimization：Tree implementation

**Tree implementation of barrier**

<img src="https://raw.githubusercontent.com/YijiaJin/Plot/master/treebarrier.png" style="zoom:65%">

* Combining trees make better use of parallelism in interconnect topologies
  * lg(P) span (latency)
  * Strategy makes less sense on a bus (all traffic still serialized on single shared bus)

* Barrier acquire: when processor arrives at barrier, performs increment of parent counter
* Barrier release: beginning from root, notify children of release

